{"root":{"data":{"text":"深度学习","expandState":"expand"},"children":[{"data":{"text":"理论基础","layout":null,"expandState":"expand"},"children":[{"data":{"text":"基础篇","hyperlink":"https://github.com/PaddlePaddle/awesome-DeepLearning/tree/master/docs/tutorials/deep_learning","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"神经元与感知机","hyperlink":"https://github.com/PaddlePaddle/awesome-DeepLearning/tree/master/docs/tutorials/deep_learning/basic_concepts","hyperlinkTitle":"","layout":null}},{"data":{"text":"向量距离与相似度","layout":null},"children":[{"data":{"text":"距离计算","layout":null},"children":[{"data":{"text":"曼哈顿距离","layout":null}},{"data":{"text":"欧式距离","layout":null}},{"data":{"text":"KL散度","layout":null}}]},{"data":{"text":"相似度函数","layout":null},"children":[{"data":{"text":"余弦相似度","layout":null}}]}]},{"data":{"text":"评估指标","expandState":"collapse","layout":null},"children":[{"data":{"text":"精度，Accuracy","layout":null}},{"data":{"text":"精准率，Precision、查准率","layout":null}},{"data":{"text":"召回率","layout":null}},{"data":{"text":"F1 值","layout":null}},{"data":{"text":"mAP：多标签图像分类任务\nmean Average Precision","font-weight":"bold","background":"#ffff00","note":"多标签图像分类任务中图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean accuracy。\n\n假设我们的输入样本中有某个类别的10个目标，我们最终预测得到了8个目标。其中6个目标预测正确（TP），2个目标预测错误（FP），4个目标没有预测到（FN）。则准确率和召回率的计算结果如下所示：\n\n* 准确率：6/（6+2） = 6/8 = 75%\n* 召回率：6/（6+4） = 6/10 = 60%","layout":null,"hyperlink":"https://blog.csdn.net/u014203453/article/details/77598997","hyperlinkTitle":""},"children":[{"data":{"text":"纵坐标：准确率（Precision）","layout":null}},{"data":{"text":"横坐标：召回率（Recall）","layout":null}},{"data":{"text":"P-R曲线","layout":null}}]},{"data":{"text":"GAN评价指标","layout":null},"children":[{"data":{"text":"IS（Inception Score）","layout":null}},{"data":{"text":"FID（Fréchet Inception Distance）","layout":null}}]},{"data":{"text":"Perplexity","layout":null}},{"data":{"text":"机器翻译评估","layout":null},"children":[{"data":{"text":"BLEU","layout":null}},{"data":{"text":"ROUGE ","layout":null}}]}]},{"data":{"text":"奥卡姆剃刀原理","note":"应该优先使用较为简单的公式或者原理，而不是复杂的。\n\n应用到机器学习任务中，可以通过减小模型的复杂度来降低过拟合的风险，即模型在能够较好拟合训练集（经验风险）的前提下，尽量减小模型的复杂度（结构风险）。","layout":null}}]},{"data":{"text":"损失函数\nLoss Function","layout":null},"children":[{"data":{"text":"CrossEntropyLoss","layout":null},"children":[{"data":{"text":"适用于单分类，预测输出是 onehot","layout":null}}]},{"data":{"text":"多分类：BCELoss","layout":null}},{"data":{"text":"MSE，均方差","layout":null},"children":[{"data":{"text":"常用于回归预测任务中","layout":null}}]},{"data":{"text":"CTC","note":"CTC 算法主要用来解决神经网络中标签和预测值无法对齐的情况，通常用于文字识别以及语音等序列学习领域。","layout":null}}]},{"data":{"text":"优化器","layout":null,"expandState":"collapse","priority":1},"children":[{"data":{"text":"深度学习主流模型与对应的优化器","progress":3,"layout":null}},{"data":{"text":"梯度下降法：对损失函数计算梯度（求导）","background":"#ffff00","layout":null,"expandState":"expand"},"children":[{"data":{"text":"批量梯度下降（batch gradient descent,BGD）","layout":null}},{"data":{"text":"随机梯度下降（ stotastic gradient descent, SGD ）","layout":null},"children":[{"data":{"text":"解决了BGD的计算冗余，收敛速度更快","layout":null}},{"data":{"text":"高方差的特点，可以跳到新的潜在的可能更好的局部最优点","layout":null}}]},{"data":{"text":"Mini-batch梯度下降","note":"MBGD 是训练神经网络时的常用方法，而且通常即使实际上使用的是 MBGD，也会使用 SGD 这个词来代替。","layout":null,"priority":2,"expandState":"expand"},"children":[{"data":{"text":"batch_size","expandState":"expand","hyperlink":"https://www.cnblogs.com/brt2/p/15641366.html","hyperlinkTitle":"cnblog: 优化器-SDG: batch_size的选择","layout":null},"children":[{"data":{"text":"优势","layout":null},"children":[{"data":{"text":"合理利用内存容量","layout":null}},{"data":{"text":"更好的处理非凸的损失函数","background":"#ffff00","note":"`batch`相当于人为引入修正梯度上的采样噪声，使“一路不通找别路”更有可能搜索最优值。","layout":null}}]},{"data":{"text":"batch size越大","layout":null,"expandState":"expand"},"children":[{"data":{"text":"批次越少，相同数据量的数据处理速度加快","note":"提高了内存的利用率，大矩阵乘法的并行化效率提高。","layout":null}},{"data":{"text":"下降方向越准，引起的训练震荡越小","layout":null}},{"data":{"text":"精度下降，想要达到相同精度所需要的epoch数量越来越多","layout":null}}]},{"data":{"text":"batch size越小","layout":null},"children":[{"data":{"text":"对数据的利用越充分，浪费的数据量越少","layout":null}},{"data":{"text":"但批次会很多，训练会更耗时","layout":null}},{"data":{"text":"算法容易修正方向导致不收敛，\n或者需要经过很大的epoch才能收敛","layout":null}}]}]},{"data":{"text":"学习率的选择","note":"可以通过“自适应学习率”优化器解决","layout":null},"children":[{"data":{"text":"太小","layout":null},"children":[{"data":{"text":"收敛非常缓慢","layout":null}},{"data":{"text":"很可能会陷入局部最优点","layout":null}}]},{"data":{"text":"太大: 阻碍收敛，导致损失函数在最优点附近震荡甚至发散","progress":3,"layout":null}},{"data":{"text":"鞍点","progress":3,"layout":null}}]}]},{"data":{"text":"Momentum","progress":3,"priority":3,"layout":null},"children":[{"data":{"text":"SGD-M, SGD with momentum","note":"在SGD基础上引入了**一阶动量**","layout":null}},{"data":{"text":"SGD-M缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题","layout":null}},{"data":{"text":"当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡","layout":null}}]},{"data":{"text":"NAG","priority":9,"expandState":"collapse","layout":null},"children":[{"data":{"text":"NAG不计算当前位置的梯度方向","layout":null},"children":[{"data":{"text":"在先前的累积梯度方向上进行大幅度的跳跃","layout":null}},{"data":{"text":"评估这个梯度并做一下修正","layout":null}}]},{"data":{"text":"有利于跳出当前局部最优的沟壑，寻找新的最优值，但收敛速度慢","layout":null}}]}]},{"data":{"text":"自适应学习率","note":"二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。\n\nSGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。因此，Adagrad 非常适用于稀疏数据。","expandState":"expand","layout":null},"children":[{"data":{"text":"AdaGrad","layout":null},"children":[{"data":{"text":"Adagrad 能够大幅提高 SGD 的鲁棒性","layout":null}},{"data":{"text":"Adagrad 非常适用于稀疏数据","font-weight":"bold","layout":null}},{"data":{"text":"大多数实现使用一个默认值 0.01","layout":null}}]},{"data":{"text":"AdaDelta","note":"由于AdaGrad单调递减的学习率变化**过于激进*，考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。","layout":null},"children":[{"data":{"text":"不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度","layout":null}},{"data":{"text":"避免了二阶动量持续累积、导致训练过程提前结束的问题","layout":null}}]},{"data":{"text":"RMSProp","note":"RMSProp 算法（Hinton，2012）修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。","layout":null},"children":[{"data":{"text":"将AdaGrad梯度累积改为：指数加权的移动平均值","layout":null}},{"data":{"text":"RMSProp 与 Adadelta 的移动均值更新方式十分相似","layout":null}}]},{"data":{"text":"Adam","font-weight":"bold","layout":null},"children":[{"data":{"text":"Adam使用动量和自适应学习率来加快收敛速度","note":null,"layout":null}},{"data":{"text":"Adam=RMSProp+Momentum","note":"SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量（二阶矩估计）。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。","layout":null},"children":[{"data":{"text":"二阶动量是固定时间窗口内的累积","layout":null}}]},{"data":{"text":"优势","layout":null},"children":[{"data":{"text":"能自然地实现步长退火过程（自动调整学习率）","layout":null}},{"data":{"text":"防止梯度的振荡和在鞍点的静止","layout":null}},{"data":{"text":"实现简单，计算高效，对内存需求少","layout":null}},{"data":{"text":"超参数具有很好的解释性，且通常无需调整或仅需很少的微调","layout":null}},{"data":{"text":"很适合应用于大规模的数据及参数的场景","font-weight":"bold","layout":null}},{"data":{"text":"适用于不稳定目标函数","layout":null}},{"data":{"text":"适用于梯度稀疏或梯度存在很大噪声的问题","layout":null}}]},{"data":{"text":"缺点","expandState":"expand","layout":null},"children":[{"data":{"text":"可能不收敛：二阶动量是固定时间窗口内的累积，因数据变化可能导致无法收敛","layout":null}},{"data":{"text":"可能错过全局最优解","note":"自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。","layout":null}}]}]},{"data":{"text":"Nadam","priority":9,"layout":null}},{"data":{"text":"AMSGrad","layout":null},"children":[{"data":{"text":"解决基于Adam的优化器的收敛问题","layout":null}},{"data":{"text":"AMSGRAD不增加步长，避免了ADAM和RMSPROP算法的缺陷。","layout":null}}]},{"data":{"text":"AdaBound","note":"AdaBound在最开始表现的像Adam，因为最开始学习率的边界对更新公式影响很小，渐渐的表现的像SGD+momentum，因为学习率逐渐被限制住了。","progress":3,"layout":null},"children":[{"data":{"text":"训练速度比肩Adam","layout":null}},{"data":{"text":"性能媲美SGD","layout":null},"children":[{"data":{"text":"SGD的问题是前期收敛速度慢","note":"SGD前期收敛慢的原因： SGD在更新参数时对各个维度上梯度的放缩是一致的，并且在训练数据分布极不均很时训练效果很差。","layout":null}},{"data":{"text":"Adam、AdaGrad、RMSprop","note":"这些自适应的优化算法虽然可以在训练早期展现出快速的收敛速度，但其在测试集上的表现却会很快陷入停滞，并最终被 SGD 超过。","layout":null}}]},{"data":{"text":"具体做法是对学习率进行动态裁剪","layout":null}}]},{"data":{"text":"AdamW","layout":null},"children":[{"data":{"text":"在损失函数里面加入了L2正则项","layout":null}},{"data":{"text":"BERT,XLNET,ELECTRA等主流的NLP模型，都是用了AdamW优化器","layout":null}}]}]}]},{"data":{"text":"激活函数","expandState":"expand","layout":null,"note":"激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数。"},"children":[{"data":{"text":"Sigmoid/Tanh","layout":null},"children":[{"data":{"text":"缺点","layout":null},"children":[{"data":{"text":"计算量较大","layout":null}},{"data":{"text":"梯度消失问题","font-weight":"bold","layout":null}}]},{"data":{"text":"输出：0.98\n常用于最后一层，表示可能性（-1, 1）","layout":null}}]},{"data":{"text":"ReLU\nRectified Linear Unit","layout":null,"image":"https://img2020.cnblogs.com/blog/881857/202004/881857-20200423162728080-1558585163.png","imageTitle":"","imageSize":{"width":191,"height":51}},"children":[{"data":{"text":"优势","layout":null,"priority":4},"children":[{"data":{"text":"计算复杂度最低","layout":null}},{"data":{"text":"不存在梯度消失","layout":null}}]},{"data":{"text":"派生：解决停止学习问题","note":"一个简单的例子是使用 ReLU 时，如果第一层的输出刚好全部都是负数，那隐藏值则全部为 0，导函数值也为 0，不管再怎么训练参数都不会变化。","layout":null},"children":[{"data":{"text":"LeakyReLU ","layout":null}},{"data":{"text":"ELU","layout":null}}]}]},{"data":{"text":"Softmax ","layout":null},"children":[{"data":{"text":"一般用于多分类问题，放在最后一层的函数","layout":null,"font-weight":"bold"}},{"data":{"text":"输出：[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n用于在分类的时候判断哪个类别可能性最大","layout":null}}]}]},{"data":{"text":"模型调优","expandState":"expand","layout":null},"children":[{"data":{"text":"学习率衰减方法","progress":3,"layout":null},"children":[{"data":{"text":"loss自适应衰减（Reduce On Plateau）","note":"一旦模型表现不再提升，将学习率降低 2-10 倍对模型的训练往往有益。","layout":null}}]},{"data":{"text":"注意力机制","progress":3,"layout":null}},{"data":{"text":"泛化模型-正则化-regularization\n(防止过拟合)","expandState":"expand","layout":null,"hyperlink":"https://blog.csdn.net/heyongluoyao8/article/details/49429629","hyperlinkTitle":""},"children":[{"data":{"text":"数据集扩增","layout":null}},{"data":{"text":"L2 正则化（权重衰减）","layout":null}},{"data":{"text":"使用 Dropout","layout":null}},{"data":{"text":"BatchNorm 正规化批次","layout":null}},{"data":{"text":"提早停止 (Early Stopping)","layout":null,"note":"提前停止是一种交叉验证的策略，即把一部分训练集保留作为验证集。当看到验证集上的性能变差时，就立即停止模型的训练。"}},{"data":{"text":"随机池化","layout":null}}]},{"data":{"text":"batch size: \n为什么10000样本训练1次会比100样本训练100次收敛慢呢？","progress":3,"note":"由于计算量是线性的，前者的计算量是后者的100倍，但均值标准差只比后者降低了10倍，那么在相同的计算量下（同样训练10000个样本），小样本的收敛速度是远快于使用整个样本集的。","layout":null}},{"data":{"text":"参数初始化","layout":null}}]},{"data":{"text":"归一化算法","layout":null,"progress":3}}]},{"data":{"text":"常用模型","layout":null},"children":[{"data":{"text":"LeNet","layout":null},"children":[{"data":{"text":"Sigmoid","layout":null}}]},{"data":{"text":"CNN","layout":null},"children":[{"data":{"text":"图像分类：图片中仅存在单个对象\n不支持多物体识别","layout":null}},{"data":{"text":"无法定位对象","layout":null}}]},{"data":{"text":"经典Backbone","expandState":"expand","hyperlink":"https://blog.csdn.net/kuweicai/article/details/102789420","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"AlexNet, 2012","layout":null},"children":[{"data":{"text":"5 层卷积 + 3 层全连接","layout":null}},{"data":{"text":"使用 ReLU 作为激活函数","layout":null}},{"data":{"text":"使用 数据增强 和 dropout 来解决过拟合问题","layout":null}},{"data":{"text":"用最大池化取代平均池化，避免平均池化的模糊化效果，\n并且在池化的时候让步长比池化核的尺寸小","layout":null}},{"data":{"text":"提出了LRN层，对局部神经元的活动创建竞争机制","layout":null}}]},{"data":{"text":"VGG 16/19, 2014","layout":null},"children":[{"data":{"text":"选用比较小的卷积核（3x3）","note":"两个3x3的感受野和一个5x5的感受野的大小相同，但是计算量却小了很多","layout":null}},{"data":{"text":"证明提高网络的深度能提高精度","layout":null}}]},{"data":{"text":"GoogLeNet, 2014","layout":null},"children":[{"data":{"text":"采用辅助分类器的方式来解决梯度消失的问题","layout":null}},{"data":{"text":"用 Ave Pool 替代FC全连接层","layout":null}}]},{"data":{"text":"ResNet, 2015, 何凯明\n残差网络","layout":null},"children":[{"data":{"text":"用 Ave Pool 替代FC全连接层","layout":null}},{"data":{"text":"ResNet 中只有开头和结尾的位置有 pooling 层，中间是没有的，\n因为 ResNet 中间采用了 stride 为2的卷积操作，取代了 pooling 层的作用","layout":null}},{"data":{"text":"解决梯度消失：identity shortcut connection，跨层向前传播","layout":null}},{"data":{"text":"变种","expandState":"expand","layout":null},"children":[{"data":{"text":"DenseNet, 2017","layout":null}},{"data":{"text":"ResNeSt, 2020","layout":null}},{"data":{"text":"Res2Net, 2019","layout":null}}]}]},{"data":{"text":"Inception","layout":null}},{"data":{"text":"Darknet-53 (YOLO-v3)","layout":null}},{"data":{"text":"EfficientNet","layout":null}}]},{"data":{"text":"对象识别模型\nObject Detection Model","hyperlink":"https://www.cnblogs.com/zkweb/p/14048685.html","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"RCNN","layout":null}},{"data":{"text":"Fast RCNN","layout":null}},{"data":{"text":"Faster RCNN","hyperlink":"https://www.cnblogs.com/zkweb/p/14078501.html","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"识别人脸位置与是否戴口罩","layout":null}}]},{"data":{"text":"YOLO","layout":null}}]}]},{"data":{"text":"迁移学习","layout":null},"children":[{"data":{"text":"两种模式"},"children":[{"data":{"text":"finetuning，重新训练模型全部参数","layout":null}},{"data":{"text":"feature extraction，仅改变最后一层参数","layout":null}}]},{"data":{"text":"预训练模型"},"children":[{"data":{"text":"两个阶段"},"children":[{"data":{"text":"预训练阶段（pre-training）","note":"一般会在超大规模的语料上，采用无监督（unsupervised）或者弱监督（weak-supervised）的方式训练模型。"}},{"data":{"text":"微调（fune-tuning）"}}]}]}]},{"data":{"text":"API","expandState":"expand"},"children":[{"data":{"text":"PyTorch","expandState":"collapse","layout":null},"children":[{"data":{"text":"utils.data","layout":null},"children":[{"data":{"text":"DataSet","expandState":"expand","hyperlink":"https://blog.csdn.net/cxx654/article/details/112131717","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"data.random_split()","hyperlink":"https://blog.csdn.net/qq_42951560/article/details/115445317","hyperlinkTitle":"","layout":null}}]},{"data":{"text":"DataLoader","layout":null},"children":[{"data":{"text":"pin_memory","hyperlink":"https://blog.csdn.net/real_ilin/article/details/105320344","hyperlinkTitle":"","layout":null}}]},{"data":{"text":"数据维度转换：torchvision.utils.make_grid(images, nrow=16).permute(1,2,0)","layout":null}}]},{"data":{"text":"torchvision","layout":null},"children":[{"data":{"text":"transforms\n图像预处理","layout":null},"children":[{"data":{"text":"数据增强","expandState":"collapse","layout":null},"children":[{"data":{"text":"transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), # 将原始图像resize","layout":null}},{"data":{"text":"transforms.RandomRotation(degrees=15),# 将图像在-15-15之间随机旋转","layout":null}},{"data":{"text":"transforms.ColorJitter(),# 修改修改亮度、对比度和饱和度","layout":null}},{"data":{"text":"transforms.RandomResizedCrop(224),","layout":null}},{"data":{"text":"transforms.RandomHorizontalFlip(), # 依概率将图像水平翻转 概率默认为0.5","layout":null}}]}]},{"data":{"text":"ImageFolder","note":"from torchvision.datasets import ImageFolder","layout":null}}]},{"data":{"text":"训练流程","expandState":"expand","layout":null},"children":[{"data":{"text":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","layout":null}},{"data":{"text":"定义Dataset，分割训练集/验证集，显示图像","expandState":"expand","layout":null},"children":[{"data":{"text":"数据的查看与整理","layout":null}},{"data":{"text":"定义batch，DataLoader加载Dataset","layout":null}}]},{"data":{"text":"定义模型","layout":null},"children":[{"data":{"text":"基于torchvision模型，迁移、修改","layout":null},"children":[{"data":{"text":"参数冻结：require_grad=False","layout":null}},{"data":{"text":"优化器","layout":null}}]}]},{"data":{"text":"自动调整学习率？optim.lr_scheduler","layout":null}},{"data":{"text":"训练函数","layout":null}},{"data":{"text":"测试函数","layout":null}}]},{"data":{"text":"优化器optim","layout":null},"children":[{"data":{"text":"torch.optim.lr_scheduler\n自动调整学习率","hyperlink":"https://blog.csdn.net/weixin_48249563/article/details/115031005","hyperlinkTitle":"","layout":null}}]},{"data":{"text":"19种损失函数","hyperlink":"https://blog.csdn.net/zhangxb35/article/details/72464152","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"return：Tensor","layout":null}}]},{"data":{"text":"迁移学习","layout":null},"children":[{"data":{"text":"实例：基于Resnet-18 实战Cifar-10","hyperlink":"https://blog.csdn.net/taxuezcy/article/details/86649374","hyperlinkTitle":"","layout":null}},{"data":{"text":"加载预训练模型","hyperlink":"https://blog.csdn.net/Vivianyzw/article/details/81061765","hyperlinkTitle":"","layout":null},"children":[{"data":{"text":"strict=False详解","hyperlink":"https://blog.csdn.net/hungryof/article/details/81364487","hyperlinkTitle":"","layout":null}}]}]},{"data":{"text":"Model网络层","expandState":"expand","layout":null},"children":[{"data":{"text":"Conv2d, 卷积层","layout":null}},{"data":{"text":"池化层","layout":null},"children":[{"data":{"text":"MaxPool2d","layout":null}},{"data":{"text":"AvgPool2d","layout":null}}]},{"data":{"text":"Linear，全连接层","layout":null}},{"data":{"text":"Dropout","note":"防止过拟合","layout":null}},{"data":{"text":"BN层：BatchNorm2d","note":"在卷积神经网络的卷积层之后总会添加BatchNorm2d进行数据的归一化处理，这使得数据在进行Relu之前不会因为数据过大而导致网络性能的不稳定。","layout":null}}]}]},{"data":{"text":"Keras","layout":null}},{"data":{"text":"Paddle","layout":null}},{"data":{"text":"MXNet","layout":null}}]},{"data":{"text":"NLP预训练"},"children":[{"data":{"text":"subword分词器"},"children":[{"data":{"text":"构造词表"}},{"data":{"text":"面临若干不足"},"children":[{"data":{"text":"OOV：无法处理词表外的词"}},{"data":{"text":"低频词汇训练不足"}},{"data":{"text":"衍生词汇，训练冗余\nlook-looks-looking-looked","layout":null}}]},{"data":{"text":"进阶：Subword Tokenization方法","note":"可以用更小的词片段来组成更大的词：\n“unfortunately” = “un” + “for” + “tun” + “ate” + “ly”"},"children":[{"data":{"text":"Byte Pair Encoding (BPE)"}},{"data":{"text":"WordPiece","note":"Google的Bert模型在分词的时候使用的是WordPiece算法。","priority":3}},{"data":{"text":"Unigram Language Model (ULM)"}}]}]},{"data":{"text":"ELMo, 2018"},"children":[{"data":{"text":"Word Embedding：词嵌入 (词向量)","note":"最简单的理解就是：将词进行向量化表示，实体的抽象成了数学描述，就可以进行建模。\n\n但是Word Embedding无法解决多义词的问题，同一个词在不同的上下文中表示不同的意思，但是在Word Embedding中一个词只有一个表示。"},"children":[{"data":{"text":"传统算法","note":null,"hyperlink":null,"hyperlinkTitle":null},"children":[{"data":{"text":"word2vec"}},{"data":{"text":"glove"}}]},{"data":{"text":"\"静态\"的","note":"在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。"}}]},{"data":{"text":"可以根据上下文，动态调整embedding","font-weight":"bold"}},{"data":{"text":"模型结构"},"children":[{"data":{"text":"得到word embedding"}},{"data":{"text":"送入双向LSTM模型中"}},{"data":{"text":"Softmax归一化"}}]},{"data":{"text":"缺点"},"children":[{"data":{"text":"使用LSTM提取特征，而LSTM提取特征的能力弱于Transformer","font-weight":"bold"}},{"data":{"text":"使用向量拼接方式融合上下文特征，效果一般"}},{"data":{"text":"训练时间长，这也是RNN的本质导致的"}}]}]},{"data":{"text":"Transformer "},"children":[{"data":{"text":"架构：注意力机制","note":"编码器和解码器没有采用 RNN 或 CNN 等网络架构，而是采用完全依赖于注意力机制的架构。"}},{"data":{"text":"优点"},"children":[{"data":{"text":"改进了RNN被人诟病的训练慢的特点，\n利用self-attention可以实现快速并行"}}]},{"data":{"text":"组成结构"},"children":[{"data":{"text":"Embedding","expandState":"expand"},"children":[{"data":{"text":"Input Embedding"}},{"data":{"text":"Position Encoding"}}]},{"data":{"text":"Encoder"},"children":[{"data":{"text":"self-attention，自注意力机制","font-weight":"bold"}},{"data":{"text":"Multi-head Attention"}},{"data":{"text":"残差连接"}},{"data":{"text":"Layer Normalization"}},{"data":{"text":"Feed Forward"}}]},{"data":{"text":"Decoder"},"children":[{"data":{"text":"传统的 Seq2Seq","note":"传统 Seq2Seq 中 Decoder 使用的是 RNN 模型，因此在训练过程中输入因此在训练过程中输入t时刻的词，模型无论如何也看不到未来时刻的词，因为循环神经网络是时间驱动的"}},{"data":{"text":"Masked Self-Attention"}}]},{"data":{"text":"Softmax"}}]}]},{"data":{"text":"Transformer-XL"},"children":[{"data":{"text":"Segment-Level 循环机制","note":"Transformer-XL能够建模更长的序列依赖，比RNN长80%，比Vanilla Transformer长450%。同时具有更快的评估速度，比Vanilla Transformer快1800+倍。","font-weight":"bold"}},{"data":{"text":"相对位置编码","font-weight":"bold"}}]},{"data":{"text":"Longformer"},"children":[{"data":{"text":"Longformer Attention"}}]},{"data":{"text":"GPT，2018"},"children":[{"data":{"text":" 无监督的预训练"}}]},{"data":{"text":"XLNet"},"children":[{"data":{"text":"从AR和AE模型到XLNet模型"},"children":[{"data":{"text":"自回归模型（Autoregressive Model, AR）"}},{"data":{"text":"自编码模型（Autoencoding Model, AE）"}}]},{"data":{"text":"引入Transformer-XL的segment循环机制"}},{"data":{"text":"与BERT不同，它使用了相对编码"},"children":[{"data":{"text":"模型的泛化效果会更好;"}},{"data":{"text":"在微调任务上，它支持超过两个segment输入的下游任务"}}]}]},{"data":{"text":"BERT, 2018","note":"该模型在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。","font-weight":"bold"},"children":[{"data":{"text":"Embedding"},"children":[{"data":{"text":"Token Embeddings是词向量"}},{"data":{"text":"Segment Embeddings用来区别两种句子"}},{"data":{"text":"Position Embeddings"}}]},{"data":{"text":"Transformer Encoder"}},{"data":{"text":"注意力六种模式"}},{"data":{"text":"BERT,GPT,ELMO的区别"},"children":[{"data":{"text":"BERT使用的是双向的Transformer（只有BERT在所有的层考虑了左右上下文）；\nOpenAI GPT使用的是从左到右的Transformer；\nELMo使用的是单独的从左到右和从右到左的LSTM拼接而成的特征","layout":null}},{"data":{"text":"BERT 比 ELMo 效果好的原因"},"children":[{"data":{"text":"LSTM 抽取特征的能力远弱于 Transformer"}},{"data":{"text":"拼接方式双向融合的特征融合能力偏弱"}},{"data":{"text":"BERT 的训练数据以及模型参数均多于 ELMo"}}]},{"data":{"text":"BERT 相较于原来的 RNN、LSTM 可以做到并发执行"}},{"data":{"text":"相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。"}}]},{"data":{"text":"衍生"},"children":[{"data":{"text":"SpanBERT"}},{"data":{"text":"RoBERTa"}},{"data":{"text":"KBERT"},"children":[{"data":{"text":"知识图谱"}}]},{"data":{"text":"ALBERT","note":"谷歌的研究者设计了一个精简的BERT（A Lite BERT，ALBERT），参数量远远少于传统的 BERT 架构。BERT (Devlin et al., 2019) 的参数很多，模型很大，内存消耗很大，在分布式计算中的通信开销很大。但是 BERT 的高内存消耗边际收益并不高，如果继续增大 BERT-large 这种大模型的隐含层大小，模型效果不升反降。\n\n启发于 mobilenet，ALBERT 通过两个参数削减技术克服了扩展预训练模型面临的主要障碍...\n\n两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。","font-weight":"bold"},"children":[{"data":{"text":"对嵌入参数化进行因式分解"}},{"data":{"text":"跨层参数共享"}},{"data":{"text":"Sentence-order prediction (SOP)来取代NSP"}},{"data":{"text":"No Dropout","note":"RoBERTA 指出 BERT 一系列模型都是” 欠拟合” 的，所以干脆直接关掉 dropout, 那么在 ALBERT 中也是去掉 Dropout 层可以显著减少临时变量对内存的占用。同时论文发现，Dropout 会损害大型 Transformer-based 模型的性能。"}}]}]}]},{"data":{"text":"ELECTRA"}},{"data":{"text":"ERINE","note":"ERINE是百度发布一个预训练模型，它通过引入三种级别的Knowledge Masking帮助模型学习语言知识，在多项任务上超越了BERT。"},"children":[{"data":{"text":"ERNIE 3.0"}},{"data":{"text":"ERNIE-Gram"}},{"data":{"text":"ERNIE-Doc"}},{"data":{"text":"THU-ERNIE","note":"该模型是由清华大学提供的，为区别百度的ERNIE，故本文后续将此模型标记为THU-ERNIE。"},"children":[{"data":{"text":"知识图谱"}}]}]},{"data":{"text":"Performer"}}]}]},"template":"default","theme":"fresh-blue-compat","version":"1.4.43"}