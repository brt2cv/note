<!--
+++
title       = "【入门】机器学习基础理论"
description = "1. 线性回归; 2. 其他概念; 3. 岭回归（Ridge Regression）; 4. 逻辑回归; 5. KNN; 6. 决策树; 7. 集成学习; 8. 贝叶斯分析; 9. 聚类算法"
date        = "2021-12-21"
tags        = []
categories  = ["7-理论知识","71-机器学习"]
series      = []
keywords    = []
weight      = 5
toc         = true
draft       = false
+++ -->

[TOC]

---

> [网易云课堂: 覃秉丰-零基础搞定机器学习及机器学习](https://study.163.com/course/courseMain.htm?courseId=1004111045)

## 1. 线性回归

回归分析最典型的例子就是“房价预测”。

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153539451-1278729629.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\0_1077750-20190223092904251-1368232387.jpg -->

### 1.1. 代价函数

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153540403-1590934133.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\1_1077750-20190223093325536-2053632256.jpg -->

#### 1.1.1. 相关系数

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153543013-760008423.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\2_1077750-20190223093606599-904528023.jpg -->

#### 1.1.2. 决定系数

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153544081-1747412133.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\3_1077750-20190223093713422-1228922924.jpg -->

y为真实值，加 - 表示均值，加 ^ 表示预测值。

### 1.2. 梯度下降法

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153545893-1757385336.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\4_1077750-20190223094301969-1028735165.jpg -->

#### 1.2.1. 用梯度下降法求解线性回归

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153546682-1789328676.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\5_1077750-20190223094555309-1740663066.jpg -->

对代价函数J(θ)的求偏导，并利用梯度下降法更新权值，不断迭代……

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153547202-165145791.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\6_1077750-20190223094947115-359452614.jpg -->

### 1.3. 多元线性回归

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153547612-514853497.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\7_1077750-20190223105420279-47750150.jpg -->

模型方程，以及代价函数：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153548134-1980528306.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\8_1077750-20190223105517917-696247129.jpg -->

运用梯度下降法（求偏导，迭代更新权值）：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153548358-1606652634.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\9_1077750-20190223105627013-1361209443.jpg -->

### 1.4. 多项式回归

### 1.5. 标准方程法

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153548787-1890477672.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\10_1077750-20190223124543187-474235993.jpg -->

例如：以下为多元房价特征，进行房价预测：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153549271-289290560.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\11_1077750-20190223124621986-2020660227.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153549648-611222215.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\12_1077750-20190223124747869-735457532.jpg -->

这里，将多个特征作为方程的因变量（多元特征向量），对w求导时就涉及到向量的导数：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153550070-1183349742.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\13_1077750-20190223125104644-1403730756.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153550331-298691786.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\14_1077750-20190223125251247-295045655.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153550727-1191900206.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\15_1077750-20190223125441848-1819805382.jpg -->

#### 1.5.1. 对比梯度下降法与标准方程法

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153550990-204441759.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\16_1077750-20190223125555448-1550907413.jpg -->

## 2. 其他概念

### 2.1. 数据归一化

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153551297-1113031624.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\17_1077750-20190223130519628-1096056531.jpg -->

#### 2.1.1. 均值标准化

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153551666-1263127767.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\18_1077750-20190223130608947-1762346605.jpg -->

### 2.2. 交叉验证法

适用于数据较少时对特征的训练

### 2.3. 过拟合

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153552018-1469262171.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\19_1077750-20190223130709637-175370810.jpg -->

### 2.4. 正则化（Regularized）

用于防止过拟合

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153552311-427139310.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\20_1077750-20190223130849267-807207084.jpg -->

## 3. 岭回归（Ridge Regression）

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153552624-102664112.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\21_1077750-20190223131206481-1510139807.jpg -->

代价函数，采用L2正则化：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153552942-1136613103.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\22_1077750-20190223131303438-943492506.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153553278-1585646286.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\23_1077750-20190223131806200-601692881.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153553511-1008922123.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\24_1077750-20190223131858705-891039943.jpg -->

### 3.1. LESSO

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153553808-2080699367.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\25_1077750-20190223215619416-2029589467.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153554143-337469343.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\26_1077750-20190223215655082-222331977.jpg -->

## 4. 逻辑回归

Sigmoid/Logistic Function

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153554441-1488216322.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\27_1077750-20190223233144887-1676266205.jpg -->

决策边界

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153554699-1814671265.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\28_1077750-20190223233257560-922827552.jpg -->

逻辑回归的代价函数

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153555111-2144408358.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\29_1077750-20190223233417634-1539805283.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153555303-1752816143.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\30_1077750-20190223233448433-1364735294.jpg -->

利用梯度下降法求解代价函数的最小值：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153555498-80504090.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\31_1077750-20190223233622569-324586870.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153555748-518861163.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\32_1077750-20190223233651939-4104297.jpg -->

多分类问题

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153555957-1553769282.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\33_1077750-20190223234107656-326086061.jpg -->

逻辑回归的正则化

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153556285-2082344159.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\34_1077750-20190223234253613-81450079.jpg -->

### 4.1. 正确率和召回率

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153556587-2138233032.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\35_1077750-20190223234441482-1332705267.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153556819-1936590214.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\36_1077750-20190223234519634-59730998.jpg -->

正确率与召回率的指标

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153557022-418012952.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\37_1077750-20190223234629705-469593025.jpg -->

## 5. KNN

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153557270-1166410054.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\38_1077750-20190223234656696-2064295144.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153557579-1609161194.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\39_1077750-20190223234817104-1765774760.jpg -->

## 6. 决策树

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153557778-1792903785.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\40_1077750-20190223235249585-1572447495.jpg -->

### 6.1. 熵

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153558003-1412321042.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\41_1077750-20190223235732928-1596951492.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153558426-384945631.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\42_1077750-20190224000031019-516351458.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153558690-140643028.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\43_1077750-20190224000143721-1515382060.jpg -->

### 6.2. ID3算法

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153558878-1112057946.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\44_1077750-20190224000732987-1385886482.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153559295-1918234342.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\45_1077750-20190224001002317-1531949902.jpg -->

### 6.3. C4.5算法

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153559547-480070512.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\46_1077750-20190224001948656-426304534.jpg -->

### 6.4. CART算法

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153559737-1848752606.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\47_1077750-20190224002147613-313130739.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153559983-142191716.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\48_1077750-20190302083626852-363177805.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153600237-459087031.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\49_1077750-20190302083744656-670063856.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153600514-345265956.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\50_1077750-20190302083948930-1638260142.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153600866-146444549.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\51_1077750-20190302084151607-1193896282.jpg -->

### 6.5. 剪枝：预剪枝、后剪枝

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153601078-1951652611.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\52_1077750-20190302084253851-2000871509.jpg -->

### 6.6. 决策树的评价（适用领域）

有点：小规模数据集有效

缺点：

  * 处理连续变量不好
  * 类别较多时，错误增加的比较快
  * 不能处理大量数据

## 7. 集成学习

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153601308-542677905.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\53_1077750-20190302085404397-1759412652.jpg -->

### 7.1. Bagging

首先，进行一种有放回的抽样~

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153601488-1375486740.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\54_1077750-20190302085712734-1879961187.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153601715-96288177.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\55_1077750-20190302085931394-1087004276.jpg -->

每个数据集，都采用一种不同的学习算法（或者同一个算法，得到不同的模型）

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153601932-1679143395.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\56_1077750-20190302085957293-1117876223.jpg -->

效果如下：

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153602125-2070157266.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\57_1077750-20190302090249049-1005894651.jpg -->

### 7.2. 随机森林

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153602347-2001057291.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\58_1077750-20190302130820900-723827881.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153602580-478995012.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\59_1077750-20190302131038516-594806356.jpg -->

### 7.3. boosting（Adaptive Boosting，自适应增强）

学习器的重点放在“容易”出错的地方——增加出错数据的概率（样本采样的权值）。

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153602850-957052737.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\60_1077750-20190302131350537-62974567.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153603071-41740826.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\61_1077750-20190302131623276-1291821825.jpg -->

### 7.4. Stacking

使用多个不同的分类器对训练集进行预测，把预测得到的结果作为次级分类器的输入。次级分类器的输出则是整个模型的最终预测结果。

### 7.5. 集成学习的总结

——人多力量大、集众家之言，以民主的方式决定正确的预测结果。

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153603298-1145685293.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\62_1077750-20190302132955323-1175821407.jpg -->

## 8. 贝叶斯分析

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153603484-1174601462.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\63_1077750-20190302133430952-1310804684.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153603685-372856105.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\64_1077750-20190302133625498-1835126014.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153603896-1953123817.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\65_1077750-20190302133834494-1004710343.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153604077-370132443.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\66_1077750-20190302133935794-1551716959.jpg -->

### 8.1. 朴素贝叶斯

多特征时的概率计算，会导致计算量巨大……

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153604312-573341076.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\67_1077750-20190302134617318-580004469.jpg -->

朴素贝叶斯算法，会假设特征X1, X2, X3...之间是相互独立的，则

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153604480-1682441396.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\68_1077750-20190302134821995-1787331286.jpg -->

#### 8.1.1. 贝叶斯多项式模型

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153604724-308904782.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\69_1077750-20190302135413372-1436849120.jpg -->

#### 8.1.2. 伯努利模型

#### 8.1.3. 混合模型

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153604937-1612677795.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\70_1077750-20190302135133507-1682382373.jpg -->

#### 8.1.4. 高斯模型

常用于处理连续性变量。

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153605143-2059195666.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\71_1077750-20190302135234564-2035952728.jpg -->

#### 8.1.5. 大脑中的贝叶斯

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153605345-333021232.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\72_1077750-20190302135646346-1240457419.jpg -->

## 9. 聚类算法

无监督式学习——

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153605541-1076193060.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\73_1077750-20190302135814594-794745680.jpg -->

### 9.1. K-MEANS

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153605764-1750337008.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\74_1077750-20190302140142647-1978455582.jpg -->

G：归类

C：计算重心，然后调整中心点

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153605966-506492547.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\75_1077750-20190302140346704-795915918.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153606178-1955041228.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\76_1077750-20190302140436989-1621807089.jpg -->

![](https://img2020.cnblogs.com/blog/2039866/202102/2039866-20210206153606373-380634876.jpg) <!-- 零基础搞定机器学习及深度学习（覃秉丰）\77_1077750-20190302140546367-963784861.jpg -->
